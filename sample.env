# Production status determines what paths are used for the chrome drivers
# (in production, they live in a static folder in the docker image but development,
# you may download them from https://googlechromelabs.github.io/chrome-for-testing/#stable (chrome binary)
# and https://chromedriver.storage.googleapis.com/index.html?path=114.0.5735.90/ (chrome driver).
# This also determines temporary file directories and whether to use --headless arg for browser
# if 0, headless is not used and you can watch the automated browser actions in a GUI. 
PRODUCTION=1

# VERBOSE=1 to enable verbose logging (debug); =0 to use normal logging (info)
VERBOSE=1
 

# Base webdriver timeout for actions like waiting for elements 
# to appear, and basic front-end operations. Intentionally 
# separate from the timeout for upload operations which take longer. 
WEBDRIVER_TIMEOUT_SECONDS=15
# make upload timeout longer than base web driver timeout
WEBDRIVER_UPLOAD_TIMEOUT_SECONDS=30


# set FILE_SOURCE=local to use local file system
# set FILE_SOURCE=google_drive to use Google Drive as file source
# set FILE_SOURCE=s3 to use AWS S3 bucket as a file source. Must also provide AWS_S3_* environment variables. 
# if using Google Drive, you must use "Anyone with the link can view"
# setting on all shared CSV links
FILE_SOURCE=local

################################################################################
################ REQUIRED ENVIRONMENT VARIABLES FOR FILE_SOURCE=s3 ############
################################################################################

# Specify the AWS region in which your bucket lives. 
AWS_S3_REGION=us-east-2

# Specify the name of your bucket. 
# This is the bucket where you should store all of your files. 
# S3 Client will simply collect the files in the root of the bucket 
# and iterate over them to drive the automation. 
# Recommended use: 
# a single "Multi User" CSV file as described below lives in the bucket 
# and it references N other user files that are also stored in the same bucket
# with the corresponding credentials for each of those. 
AWS_S3_BUCKET=CHANGEME

################################################################################
############# END REQUIRED ENVIRONMENT VARIABLES FOR FILE_SOURCE=s3 ############
################################################################################

# NOTE: AWS Access Key ID and AWS Secret Access Key should not be stored as environment
# variables if you are running with AWS Lambda. If you are running with AWS Lambda, 
# use the IAM service to create a role that has access to your S3 bucket (specifically)
# and assign that as the execution role for your Lambda function.
# You should use these two environment variables ONLY if you are running outside of AWS Lambda
# AND you are storing your CSV files in AWS S3. 
AWS_ACCESS_KEY_ID=CHANGEME
AWS_SECRET_ACCESS_KEY=CHANGEME
 

################################################################################
############# END REQUIRED ENVIRONMENT VARIABLES FOR FILE_SOURCE=s3 ############
################################################################################

# MODE options: single_user, multi_user. single_user is default if not set. 
# each mode has corresponding required environment variables. See below. 
MODE=multi_user

################################################################################
################ REQUIRED ENVIRONMENT VARIABLES FOR single_user MODE ##########
################################################################################
# for executing automation only for a single user's account
SINGLE_USER_USERNAME=CHANGEME # for login
SINGLE_USER_PASSWORD=CHANGEME # for login

# SINGLE_USER_CSV can either be:
# 1) a copied file link from Google drive like https://drive.google.com/file/d/SOME_LONG_FILE_ID/view?usp=drive_link
# with "Anyone with the link can view" permission.  Use approach 1 if FILE_SOURCE=google_drive
# 2) a path to a local file starting with the container path used in the volume from docker-compose.yml. 
# Use approach 2 if FILE_SOURCE=local. 
# Example: if volume in docker-compose.yml is: 
# - ./files:/opt/data/files
# you could store your file in this project folder as "./files/myfile.csv" 
# and then set SINGLE_USER_CSV=/opt/data/files/myfile.csv
SINGLE_USER_CSV=CHANGEME 

##############################################################################
######## END REQUIRED ENVIRONMENT VARIABLES FOR single_user MODE ############# 
############################################################################## 


################################################################################
################ REQUIRED ENVIRONMENT VARIABLES FOR multi_user MODE ##########
################################################################################
 

# MULTI_USER_CSV can either be:
# 1) a copied file link from Google drive like https://drive.google.com/file/d/SOME_LONG_FILE_ID/view?usp=drive_link
# with "Anyone with the link can view" permission with each CSV record 
# also pointing to a Google Drive file with similar permissions.  Use approach 1 if FILE_SOURCE=google_drive.
# 2) a path to a local file starting with the container path used in the volume from docker-compose.yml. Each user
# record in the MULTI_USER_CSV must look like username,password,local_csv_path. Use approach 2 if FILE_SOURCE=local
# Example: if volume in docker-compose.yml is: 
# - ./files:/opt/data/files
# you could store your file in this project folder as "./files/multi_user_file.csv" 
# and then set MULTI_USER_CSV=/opt/data/files/multi_user_file.csv. The user records in 
# that CSV file path may each look like username,password,/opt/data/files/users/<username>.csv

MULTI_USER_CSV=CHANGEME 

##############################################################################
######## END REQUIRED ENVIRONMENT VARIABLES FOR multi_user MODE ############# 
############################################################################## 

##############################################################################
########  REQUIRED ENVIRONMENT VARIABLES FOR AWS LAMBDA EXECUTION ############# 
############################################################################## 

AWS_LAMBDA_ARN=CHANGEME
# Once you create an ECR repository and you have pushed the docker image 
# for this project into that repository, copy the :latest URI and paste that as
# the value for AWS_LAMBDA_ECR_IMAGE_URI; it is important that you use the :latest 
# image and not @sha.... because that will only pull a specific version of the image
# which will fall out of date. 
AWS_LAMBDA_ECR_IMAGE_URI=CHANGEME	


 